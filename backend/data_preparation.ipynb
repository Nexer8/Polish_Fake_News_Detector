{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook Setup"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rvdXqfSKPXc",
        "outputId": "ef6b1be0-dedd-4862-ebc6-1ad1d63684c3"
      },
      "outputs": [],
      "source": [
        "!pip install -i https://pypi.clarin-pl.eu lpmn_client -q"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cX-nXlspqesI"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxS2kfBwqdar",
        "outputId": "e7db9d53-063d-4f88-e76b-a00112b9509c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "from shutil import make_archive, rmtree\n",
        "\n",
        "import pandas as pd\n",
        "from lpmn_client import Task, download_file_as_dict, upload_file\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 10)\n",
        "pd.set_option('display.max_colwidth', 120)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_PATH = '../scraper/citations.csv'\n",
        "TEMP_DIR = 'temp_out'\n",
        "TEMP_ZIP = 'temp_zip'\n",
        "CLEAN_DATA_CSV = 'clean.csv'\n",
        "TFIDF_CSV = 'tfidf.csv'\n",
        "\n",
        "CLASS_LABELS = {\n",
        "    'Fałsz': 0,\n",
        "    'Prawda': 1,\n",
        "    'Manipulacja': 2,\n",
        "    'Nieweryfikowalne': 3\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8c1FijT1mrnh"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "-dy75juyZrT2",
        "outputId": "97eb581f-3abd-4dfc-b270-cb8ab6da4b27"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(DATA_PATH, sep=';', dtype={\n",
        "    'content': str,\n",
        "    'author': str,\n",
        "    'label': str\n",
        "})\n",
        "\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "df['label'].replace(CLASS_LABELS, inplace=True)\n",
        "df = df[(df['label'] == CLASS_LABELS['Fałsz']) | (df['label'] == CLASS_LABELS['Prawda'])]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def df_to_file(df, field):\n",
        "    if os.path.exists(TEMP_DIR):\n",
        "        rmtree(TEMP_DIR, ignore_errors=True)\n",
        "    os.mkdir(TEMP_DIR)\n",
        "    for index, _ in df.iterrows():\n",
        "        with open(f'{TEMP_DIR}/{index}.txt', 'w') as file:\n",
        "            file.write(df.at[index, field])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "73PqDGDXRSJB"
      },
      "source": [
        "# Feature Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEHv3t1ARYQP"
      },
      "outputs": [],
      "source": [
        "def count_uppercase_letters(text):\n",
        "    count = sum([1 for char in text if char.isupper()])\n",
        "    return (count / (len(text) - text.count(' '))) * 100\n",
        "\n",
        "\n",
        "def count_exclamation_marks(text):\n",
        "    count = text.count('!')\n",
        "    return (count / (len(text) - text.count(' '))) * 100\n",
        "\n",
        "\n",
        "def count_question_marks(text):\n",
        "    count = text.count('?')\n",
        "    return (count / (len(text) - text.count(' '))) * 100\n",
        "\n",
        "\n",
        "def count_quotation_marks(text):\n",
        "    count = text.count('\"')\n",
        "    return (count / (len(text) - text.count(' '))) * 100\n",
        "\n",
        "\n",
        "def count_punctuation(text):\n",
        "    count = sum([1 for char in text if char in string.punctuation])\n",
        "    return (count / (len(text) - text.count(' '))) * 100\n",
        "\n",
        "\n",
        "def count_text_length(text):\n",
        "    return len(text) - text.count(' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_query(file, lpmn_query):\n",
        "    task = Task(lpmn_query)\n",
        "    file_id = upload_file(file)\n",
        "    output_file_id = task.run(file_id)\n",
        "    return download_file_as_dict(output_file_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah8YXWTVZ0A7"
      },
      "outputs": [],
      "source": [
        "def remove_punctuation(text):\n",
        "    no_punctuation_text = ''.join(\n",
        "        [char for char in text if char not in string.punctuation])\n",
        "    return no_punctuation_text\n",
        "\n",
        "\n",
        "def parse_sentiment(data):\n",
        "    try:\n",
        "        df = pd.read_csv(StringIO(data), sep=';')\n",
        "        sentiment_value = sum([int(entry) for entry in df['Polarity'].values if\n",
        "                               (type(entry) == str and entry.isnumeric()) or isinstance(entry, (int, float, complex))])\n",
        "        positive_words = (sum([1 for entry in df['Polarity'].values if\n",
        "                               (type(entry) == str and entry.isnumeric() or isinstance(entry,\n",
        "                                                                                       (int, float, complex))) and int(\n",
        "                                   entry) > 0]) / len(df['Polarity']))\n",
        "        negative_words = (sum([1 for entry in df['Polarity'].values if\n",
        "                               (type(entry) == str and entry.isnumeric() or isinstance(entry,\n",
        "                                                                                       (int, float, complex))) and int(\n",
        "                                   entry) < 0]) / len(df['Polarity']))\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        sentiment_value = 0\n",
        "        positive_words = 0\n",
        "        negative_words = 0\n",
        "\n",
        "    return sentiment_value, positive_words, negative_words\n",
        "\n",
        "\n",
        "def parse_lemmatization(text):\n",
        "    lemmatized_text = [word.text for word in ET.fromstring(\n",
        "        text).findall('chunk/sentence/tok/lex/base')]\n",
        "    return ' '.join(lemmatized_text)\n",
        "\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = re.split('\\W+', text)\n",
        "    text = [word for word in tokens]\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    column_names = ['idx', 'ranking', 'output_phrase', 'original_phrase', 'c-value',\n",
        "                    'length', 'freq_s', 'freq_in', 'context']\n",
        "    df = pd.read_csv(StringIO(text), sep='\\t', names=column_names)\n",
        "    return df['output_phrase'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_to_file(df, 'content')\n",
        "zip_path = make_archive(TEMP_ZIP, 'zip', TEMP_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lpmn_sentiment_query = 'any2txt|wcrft2|wsd|ccl_emo({\"lang\":\"polish\"})|ccl_emo_stats({' \\\n",
        "            '\"lang\":\"polish\", \"split_paragraphs\": false})'\n",
        "sentiment_dict = execute_query(zip_path, lpmn_sentiment_query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for index, _ in df.iterrows():\n",
        "    no_punctuation_text = remove_punctuation(df.at[index, 'content'])\n",
        "    with open(f'{TEMP_DIR}/{index}.txt', 'w') as file:  \n",
        "        file.write(''.join(no_punctuation_text))\n",
        "zip_path = make_archive(TEMP_ZIP, 'zip', TEMP_DIR)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lpmn_lemmatization_query = 'any2txt|wcrft2({\"guesser\":false, \"morfeusz2\":true})'\n",
        "lemmatized_dict = execute_query(zip_path, lpmn_lemmatization_query)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean CSV Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['uppercase%'] = df['content'].apply(lambda x: count_uppercase_letters(x))\n",
        "df['exclamation_mark%'] = df['content'].apply(lambda x: count_exclamation_marks(x))\n",
        "df['question_mark%'] = df['content'].apply(lambda x: count_question_marks(x))\n",
        "df['quotation_mark%'] = df['content'].apply(lambda x: count_quotation_marks(x))\n",
        "df['punctuation%'] = df['content'].apply(lambda x: count_punctuation(x))\n",
        "df['length'] = df['content'].apply(lambda x: count_text_length(x))\n",
        "\n",
        "rows_list = []\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    try:\n",
        "        sentiment, positive_words, negative_words = parse_sentiment(sentiment_dict[f'{index}.txt'])\n",
        "        cleaned_text = parse_lemmatization(lemmatized_dict[f'{index}.txt'])\n",
        "        dictionary = {\n",
        "            'sentiment': sentiment,\n",
        "            'positive_words%': positive_words,\n",
        "            'negative_words%': negative_words,\n",
        "            'clean_text': cleaned_text\n",
        "        }\n",
        "\n",
        "        rows_list.append(dictionary)\n",
        "    except KeyError:\n",
        "        print(f'KeyError on index: {index}')\n",
        "\n",
        "df = pd.concat([df.reset_index(drop=True), pd.DataFrame(\n",
        "    rows_list).reset_index(drop=True)], axis=1)\n",
        "df.to_csv(CLEAN_DATA_CSV)\n",
        "print(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rmtree(TEMP_DIR, ignore_errors=True)\n",
        "os.remove(zip_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stopwords Removal (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df_to_file(df, 'clean_text')\n",
        "# zip_path = make_archive(TEMP_ZIP, 'zip', TEMP_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# lpmn_query = 'any2txt|morphoDita|dir|termopl2({\\\"mw\\\":false,\\\"sw\\\":\\\"/resources/termopl/termopl_sw.txt\\\",' \\\n",
        "#                 '\\\"cp\\\":\\\"/resources/termopl/termopl_cp.txt\\\"}) '\n",
        "# downloaded = execute_query(TEMP_ZIP, lpmn_query)\n",
        "# no_stopwords = remove_stopwords(downloaded[next(iter(downloaded))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5RpIZNAcSYI3"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M29--NLvSgJ7"
      },
      "source": [
        "## N-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqQnNTC3ScdN"
      },
      "outputs": [],
      "source": [
        "ngram_vect = CountVectorizer(ngram_range=(2, 2))\n",
        "ngram = ngram_vect.fit_transform(df['content'])\n",
        "ngram_df = pd.DataFrame(ngram.toarray())\n",
        "ngram_df.columns = ngram_vect.get_feature_names()\n",
        "ngram_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lYny71hQSlGb"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1AM3pFYSk1-"
      },
      "outputs": [],
      "source": [
        "clean_joined = df['clean_text']\n",
        "tfidf_vect = TfidfVectorizer(ngram_range=(1, 3))\n",
        "X_tfidf = tfidf_vect.fit_transform(clean_joined)\n",
        "X_tfidf_feat = pd.concat([df[['uppercase%', 'exclamation_mark%', 'question_mark%', 'quotation_mark%', 'punctuation%',\n",
        "                              'length', 'sentiment', 'positive_words%', 'negative_words%']].reset_index(drop=True),\n",
        "                          pd.DataFrame(X_tfidf.toarray())], axis=1)\n",
        "X_tfidf_feat.to_csv(TFIDF_CSV)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e1a030e97a7e20cc6a62d3e71aa6bb37a8ca1dc0da41b0ab8c0fbd824b017fc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
